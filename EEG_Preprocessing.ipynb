{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoahIslam/ShinozakiLabEEGMLModels/blob/main/EEG_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUL5yfS8SQ06",
        "outputId": "4dee9eb9-88b4-46cb-bda1-a70af2c92620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulHYVO9x2wA2",
        "outputId": "69674527-eef9-497f-8731-ed0076606dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.5.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from mne) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from mne) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.66.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.7.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (23.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2023.7.22)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mne\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ1YhXGLZL-3"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import mne\n",
        "import torch.nn.functional as F\n",
        "dtype = torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3J-Zrb3ZNK7"
      },
      "outputs": [],
      "source": [
        "edf_loc = \"/content/drive/MyDrive/EDF Files_copy/\"\n",
        "xcel_loc = \"/content/drive/MyDrive/EEG_Clinical_Data_Master_File_Broadcasted.xlsx\"\n",
        "\n",
        "#gets list of all files (copied from original notebook)\n",
        "def get_list_of_files(filepath):\n",
        "  entries = os.listdir(filepath)\n",
        "  files = []\n",
        "  for item in entries:\n",
        "    if os.path.isdir(os.path.join(filepath, item)):\n",
        "      files.extend(get_list_of_files(os.path.join(filepath, item)))\n",
        "  files.extend([os.path.join(filepath, x) for x in os.listdir(filepath) if os.path.isfile(os.path.join(filepath, item)) and os.path.join(filepath, item)[-4:] == \".edf\"])\n",
        "  return files\n",
        "\n",
        "files = get_list_of_files(edf_loc)\n",
        "files_1 = files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(files))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0openocxHpC",
        "outputId": "b04baeac-da90-4003-f923-54006fc1766c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTV7UQgfZ4zO"
      },
      "outputs": [],
      "source": [
        "#get xcel as csv\n",
        "def xcel_csv(file_loc):\n",
        "  df = pd.read_excel(file_loc)\n",
        "  df.to_csv('EEG_Data.csv', index=False)\n",
        "\n",
        "  return df\n",
        "\n",
        "xcel = xcel_csv(xcel_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiptkf6_akJn"
      },
      "outputs": [],
      "source": [
        "raw_data = []\n",
        "\n",
        "def to_datetime(timedate):\n",
        "  rem_paren = timedate.split('/')[-1]\n",
        "  filename = rem_paren.split(\"_\")\n",
        "  EEG_ID = filename[0]\n",
        "  date = filename[1]\n",
        "  if date[len(date)-4:len(date)] == '2019' or date[len(date)-4:len(date)] == '2020' or date[len(date)-4:len(date)] == '2018' or date[len(date)-4:len(date)] == '2017' or date[len(date)-4:len(date)] == '2016':\n",
        "    month = date[len(date)-8:len(date)-6]\n",
        "    day = date[len(date)-6:len(date)-4]\n",
        "    year = date[len(date)-4:len(date)]\n",
        "  else:\n",
        "    year = date[len(date)-8:len(date)-4]\n",
        "    month = date[len(date)-4:len(date)-2]\n",
        "    day = date[len(date)-2:len(date)]\n",
        "\n",
        "  try:\n",
        "    return (int(EEG_ID), datetime.datetime(int(year), int(month), int(day), 0, 0, 0))\n",
        "  except Exception as e:\n",
        "    return ()\n",
        "\n",
        "\n",
        "def csv_to_dict(files, df):\n",
        "  filename_to_score = {}\n",
        "  shitlist = []\n",
        "  for curr_file in files:\n",
        "    inputs = to_datetime(curr_file)\n",
        "    if inputs != ():\n",
        "      has_delirium = df.loc[(df[\"Date\"] == inputs[1]) & (df['EEG STUDY ID'] == inputs[0]), 'Delirious (0=N, 1=Y)']\n",
        "    if (len(has_delirium) == 0 or has_delirium.values[0] == '-' or has_delirium.values[0] == '?' or np.isnan(has_delirium.values[0])):\n",
        "      shitlist.append(curr_file)\n",
        "    else:\n",
        "        # Filter out Fukuoda data\n",
        "        if curr_file[-5] != 'A' and curr_file[-5] != 'B':\n",
        "            filename_to_score[curr_file] = has_delirium.values[0]\n",
        "  return filename_to_score\n",
        "\n",
        "filename_to_score = csv_to_dict(files_1, xcel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPB0Sh-5beQU"
      },
      "outputs": [],
      "source": [
        "raw_data = []\n",
        "sfreq = 500\n",
        "\n",
        "def standardize_data(data):\n",
        "    return (data - np.mean(data)) / np.std(data)\n",
        "\n",
        "def bandpass_filter(data, low_freq, high_freq, sfreq):\n",
        "    return mne.filter.filter_data(data, sfreq, l_freq=low_freq, h_freq=high_freq, verbose='ERROR')\n",
        "\n",
        "\n",
        "def edf_to_arr(edf_path):\n",
        "  try:\n",
        "    raw = mne.io.read_raw_edf(edf_path, verbose='ERROR')\n",
        "    duration = raw.times[-1]  # this gives the duration in seconds\n",
        "    if duration < 128:\n",
        "        print(f\"Data duration for {edf_path} is less than 128 seconds. Skipping...\")\n",
        "        return -1\n",
        "    raw = raw.crop(tmax=128)\n",
        "\n",
        "    raw_resampled = raw.resample(sfreq, verbose='ERROR')\n",
        "    eeg_data = raw_resampled.get_data()[0]\n",
        "    eeg_data = bandpass_filter(eeg_data, 0.5, 50, sfreq)\n",
        "    eeg_data = standardize_data(eeg_data)\n",
        "    return eeg_data.astype(np.float32)\n",
        "\n",
        "  except Exception as e:\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyKj5_sVbCBh"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "labels = []\n",
        "i = 1\n",
        "for filename, label in filename_to_score.items():\n",
        "    print(i)\n",
        "    i+=1\n",
        "    arr = edf_to_arr(filename)\n",
        "    if type(arr) != int and (label == 1 or label == 0):\n",
        "        data.append(arr)\n",
        "        labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGO_WZFSktA9"
      },
      "outputs": [],
      "source": [
        "print(len(data))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zspwm96Dm9SW"
      },
      "outputs": [],
      "source": [
        "for label in labels:\n",
        "  if label != 1 and label != 0:\n",
        "    print(label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# To save the data\n",
        "with open('data_cropped.pkl', 'wb') as f:\n",
        "    pickle.dump(data, f)\n",
        "\n",
        "with open('labels_cropped.pkl', 'wb') as f:\n",
        "    pickle.dump(labels, f)"
      ],
      "metadata": {
        "id": "-AOqRU78Pq2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you've already mounted Google Drive\n",
        "import shutil\n",
        "\n",
        "shutil.move('data_cropped.pkl', '/content/drive/MyDrive/data_cropped.pkl')\n",
        "shutil.move('labels_cropped.pkl', '/content/drive/MyDrive/labels_cropped.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rXSh0F-FfQlo",
        "outputId": "b68ba2ed-276e-4575-cb8d-5b8f5f6b3911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/labels_cropped.pkl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VeqvAGHeSv6",
        "outputId": "eaafe940-1e15-4b27-8409-681528ac2319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOgOCkcReZC_",
        "outputId": "87767af6-e3ad-4a4e-e843-2ccad0817102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3Anf_PShIFR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, stratify=labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxHPmrDgj3TG"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Combine the data and labels for easier resampling\n",
        "combined = list(zip(X_train, y_train))\n",
        "\n",
        "# Separate classes\n",
        "class_0 = [item for item in combined if item[1] == 0]\n",
        "class_1 = [item for item in combined if item[1] == 1]\n",
        "\n",
        "# Upsample the minority (this assumes class_1 is the minority. If not, switch them around.)\n",
        "class_1_upsampled = resample(class_1, replace=True, n_samples=len(class_0), random_state=42)\n",
        "\n",
        "# Combine and shuffle\n",
        "balanced_data = class_0 + class_1_upsampled\n",
        "np.random.shuffle(balanced_data)\n",
        "\n",
        "X_train_balanced = [item[0] for item in balanced_data]\n",
        "y_train_balanced = [item[1] for item in balanced_data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5ORu5uor5FL"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Separate sequences and labels\n",
        "    sequences, labels = zip(*batch)\n",
        "\n",
        "    # Get sequence lengths and sort in descending order\n",
        "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
        "    lengths, order = lengths.sort(0, descending=True)\n",
        "\n",
        "    sequences_sorted = [sequences[i] for i in order]\n",
        "    labels_sorted = torch.tensor(labels, dtype=torch.float32)[order]\n",
        "\n",
        "    # Pad sequences\n",
        "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences_sorted, batch_first=True)\n",
        "\n",
        "    return sequences_padded, lengths, labels_sorted\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FtBzTnykCYP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensors = [torch.tensor(d, dtype=torch.float32).unsqueeze(1) for d in X_train_balanced]  # Adding channel dimension\n",
        "y_train_tensors = torch.tensor(y_train_balanced, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_test_tensors = [torch.tensor(d, dtype=torch.float32).unsqueeze(1) for d in X_test]  # Adding channel dimension\n",
        "y_test_tensors = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = list(zip(X_train_tensors, y_train_tensors))\n",
        "test_dataset = list(zip(X_test_tensors, y_test_tensors))\n",
        "\n",
        "# Create dataloaders\n",
        "BATCH_SIZE = 10\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkmUl29VsAuA"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DeliriumLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(DeliriumLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 2)  # Two output units for binary classification\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Handle variable-length sequences\n",
        "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
        "        packed_out, _ = self.lstm(packed_x)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LYy2buYsEmp"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "INPUT_SIZE = 1  # 1-dimensional input (one EEG channel)\n",
        "HIDDEN_SIZE = 64\n",
        "NUM_LAYERS = 2\n",
        "LR = 0.001\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = DeliriumLSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
        "criterion = nn.CrossEntropyLoss()  # Use cross-entropy loss for binary classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqBtWceAtcDn"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for sequences, lengths, labels in train_loader:\n",
        "        sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(sequences, lengths)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for sequences, lengths, labels in test_loader:\n",
        "            sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(sequences, lengths)\n",
        "            val_loss += criterion(outputs, labels.long()).item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.long()).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss/len(test_loader):.4f}, Validation Accuracy: {100 * correct / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5g5kZa_tfr9"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sequences, lengths, labels in test_loader:\n",
        "        sequences, lengths, labels = sequences.to(device), lengths.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(sequences, lengths)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.long()).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}